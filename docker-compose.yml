services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.app
    env_file: .env
    volumes:
      - ./secrets:/run/secrets:ro
      - ./cache:/root/.cache/huggingface
      - ./models/nlp:/models/nlp:ro
    network_mode: "host"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 6
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5
    command: ["opentelemetry-instrument", "python", "-m", "app.main"]
    healthcheck:
      test: ["CMD", "python", "-m", "tools.healthcheck_app"]
      interval: 15s
      timeout: 3s
      retries: 10
      start_period: 20s

  tts:
    build:
      context: .
      dockerfile: Dockerfile.tts
    env_file: .env
    volumes:
      - ./config:/config:ro
      - ./scripts:/scripts:ro
      - ./data:/data:ro
      - ./cache:/root/.cache/huggingface
      - ./models/tts:/models/tts:ro
      # The nemo cache from the build stage can be mounted here if needed,
      # but for now the models are inside the image's /opt/nemo_cache
      # - ./nemo_cache:/opt/nemo_cache
    network_mode: "host"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=6,7
      - NEMO_CACHE_DIR=/opt/nemo_cache
    command: ["opentelemetry-instrument", "python", "-m", "uvicorn", "tts_server.api:app", "--host", "0.0.0.0", "--port", "5002"]
    healthcheck:
      test: ["CMD", "python", "-m", "tts_server.healthcheck"]
      interval: 15s
      timeout: 3s
      retries: 10
      start_period: 20s
